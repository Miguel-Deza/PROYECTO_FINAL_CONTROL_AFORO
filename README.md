# ğŸŒŸ Sistema Automatizado de Monitoreo y Control de OcupaciÃ³n en Tiempo Real Usando Transfer Learning con la Arquitectura MobileNet-SSD ğŸŒŸ

[![TÃ­tulo del video](https://github.com/user-attachments/assets/9f5a9112-8ae0-4d29-9891-6c45be0f4727)](https://www.youtube.com/watch?v=sKNYA7EUtPc)

## ğŸ“‹ Resumen

Este proyecto desarrolla un sistema de monitoreo automatizado y control de aforo en tiempo real utilizando Transfer Learning en la arquitectura MobileNet-SSD, optimizado para dispositivos de baja capacidad computacional. MobileNet-SSD equilibra precisiÃ³n y eficiencia, ideal para entornos con recursos limitados. El sistema detecta y cuenta personas en tiempo real, proporcionando datos precisos sobre el aforo. Implementado en Python y TensorFlow, el modelo se optimiza para dispositivos como Raspberry Pi. Las pruebas demuestran su efectividad en diversos escenarios, ofreciendo una soluciÃ³n viable para el control de aforo en seguridad, salud y gestiÃ³n de eventos.

## ğŸ“ IntroducciÃ³n

### â“ Problema

El crecimiento constante de la poblaciÃ³n y la creciente necesidad de mantener el control de aforo en espacios pÃºblicos y privados han impulsado la bÃºsqueda de soluciones tecnolÃ³gicas avanzadas. La capacidad de monitorear y controlar el nÃºmero de personas en un Ã¡rea especÃ­fica es crucial para garantizar la seguridad, la salud y la eficiencia en la gestiÃ³n de eventos y lugares concurridos.

### ğŸ¯ MotivaciÃ³n

Una de las principales motivaciones de este proyecto es desarrollar un sistema que pueda ser implementado en dispositivos de baja capacidad computacional, como Raspberry Pi, permitiendo asÃ­ un despliegue econÃ³mico y accesible en una variedad de entornos. La utilizaciÃ³n de Transfer Learning en la arquitectura MobileNet-SSD permite crear un sistema eficiente y preciso para la detecciÃ³n y conteo de personas en tiempo real.

### ğŸ“š Trabajos Relacionados

Trabajos previos han demostrado la eficacia de los modelos de detecciÃ³n de objetos en tareas similares, pero a menudo requieren recursos computacionales significativos que no son viables para dispositivos de baja capacidad. En este contexto, la elecciÃ³n de MobileNet-SSD es estratÃ©gica, ya que combina una arquitectura ligera con una precisiÃ³n aceptable, optimizada para funcionar en entornos con recursos limitados.

## ğŸ› ï¸ Plan General del Proyecto

El plan general seguido en este proyecto incluye la recolecciÃ³n de datos, la adaptaciÃ³n y entrenamiento del modelo utilizando Transfer Learning, y la implementaciÃ³n y prueba del sistema en dispositivos de baja capacidad.

### ğŸ§‘â€ğŸ”¬ MetodologÃ­a

La metodologÃ­a seguida en este proyecto se divide en varias etapas clave: recolecciÃ³n de datos, preprocesamiento de datos, adaptaciÃ³n y entrenamiento del modelo, implementaciÃ³n en dispositivos de baja capacidad y pruebas exhaustivas del sistema.

### ğŸ“¸ RecolecciÃ³n de Datos

Para el entrenamiento del modelo, se recolectaron imÃ¡genes de diferentes entornos que representan escenarios de aforo variados. Estas imÃ¡genes se encontraban etiquetadas para identificar y contar el nÃºmero de personas presentes en cada una. Se usÃ³ la base de datos INRIA person que proporcionaba una gran selecciÃ³n de imÃ¡genes con sus respectivas etiquetas, pero considerando que nuestro objetivo se enfocaba en el aforo de personas y no en la identificaciÃ³n de otros objetos, se usÃ³ un lote de 115 imÃ¡genes con sus respectivas etiquetas.

### ğŸ§¹ Preprocesamiento de Datos

El preprocesamiento de datos incluyÃ³ seleccionar las imÃ¡genes que contenÃ­an personas en lugar de otros objetos. Se agruparon todas esas imÃ¡genes en un solo fichero para que posteriormente sea comprimido y luego este pueda ser descomprimido con la ayuda de un script para seleccionar imÃ¡genes de entrenamiento, de validaciÃ³n y de test.

### ğŸ§  AdaptaciÃ³n y Entrenamiento del Modelo

Se utilizÃ³ la tÃ©cnica de Transfer Learning para adaptar un modelo MobileNet-SSD preentrenado a nuestro conjunto de datos especÃ­fico. La arquitectura MobileNet-SSD fue seleccionada por su equilibrio entre precisiÃ³n y eficiencia computacional. El entrenamiento se llevÃ³ a cabo en Python utilizando la biblioteca TensorFlow, aprovechando la capacidad de Transfer Learning para reducir el tiempo de entrenamiento y mejorar la precisiÃ³n en escenarios especÃ­ficos. Para la creaciÃ³n de CSV se usÃ³ otro script.

### ğŸ–¥ï¸ ImplementaciÃ³n en Dispositivos de Baja Capacidad

El modelo entrenado se optimizÃ³ para su implementaciÃ³n en dispositivos de baja capacidad computacional, como Raspberry Pi. Se realizaron ajustes en los hiperparÃ¡metros y se emplearon tÃ©cnicas de cuantizaciÃ³n para reducir el tamaÃ±o del modelo sin comprometer significativamente su precisiÃ³n.

### ğŸ§ª Pruebas y ValidaciÃ³n

El sistema se evaluÃ³ en diversos escenarios para validar su efectividad y precisiÃ³n. Se realizaron pruebas en tiempo real para verificar la capacidad del sistema de detectar y contar personas en distintos entornos y condiciones de iluminaciÃ³n. TambiÃ©n se incluyÃ³ un script para realizar las pruebas en videos, lo cual tuvo una gran acogida para demostrar los resultados. Para lo cual se usaron los scripts de Cartucho para medir el mAP y el de EdjeElectronics que proporcionaba el ejemplo de uso.

### ğŸ—ï¸ Estructura del proyecto

```
PROYECTO_FINAL_CONTROL_AFORO
â”œâ”€ colab_training
â”‚  â””â”€ PROYECTO_FINAL_ENTRENAMIENTO.ipynb
â”œâ”€ count_person_video.py
â”œâ”€ count_person_webcam.py
â”œâ”€ custom_model_lite
â”‚  â”œâ”€ detect.tflite
â”‚  â”œâ”€ labelmap.pbtxt
â”‚  â”œâ”€ labelmap.txt
â”‚  â”œâ”€ pipeline_file.config
â”‚  â””â”€ saved_model
â”‚     â”œâ”€ saved_model.pb
â”‚     â””â”€ variables
â”‚        â”œâ”€ variables.data-00000-of-00001
â”‚        â””â”€ variables.index
â”œâ”€ custom_model_lite.zip
â”œâ”€ images.zip
â”œâ”€ mylib
â”‚  â”œâ”€ centroidtracker.py
â”‚  â”œâ”€ config.py
â”‚  â”œâ”€ mailer.py
â”‚  â”œâ”€ thread.py
â”‚  â”œâ”€ trackableobject.py
â”‚  â””â”€ __pycache__
â”‚     â”œâ”€ centroidtracker.cpython-312.pyc
â”‚     â”œâ”€ centroidtracker.cpython-37.pyc
â”‚     â”œâ”€ centroidtracker.cpython-39.pyc
â”‚     â”œâ”€ config.cpython-312.pyc
â”‚     â”œâ”€ config.cpython-37.pyc
â”‚     â”œâ”€ config.cpython-39.pyc
â”‚     â”œâ”€ mailer.cpython-312.pyc
â”‚     â”œâ”€ mailer.cpython-37.pyc
â”‚     â”œâ”€ mailer.cpython-39.pyc
â”‚     â”œâ”€ thread.cpython-312.pyc
â”‚     â”œâ”€ thread.cpython-37.pyc
â”‚     â”œâ”€ thread.cpython-39.pyc
â”‚     â”œâ”€ trackableobject.cpython-312.pyc
â”‚     â”œâ”€ trackableobject.cpython-37.pyc
â”‚     â””â”€ trackableobject.cpython-39.pyc
â”œâ”€ PDF
â”‚  â””â”€ PROYECTO_FINAL_IEEE.pdf
â”œâ”€ README.md
â”œâ”€ scripts.txt
â”œâ”€ TFLite_detection_image.py
â”œâ”€ TFLite_detection_stream.py
â”œâ”€ TFLite_detection_video.py
â”œâ”€ TFLite_detection_webcam.py
â””â”€ video_test
   â”œâ”€ a.mp4
   â”œâ”€ b.mp4
   â”œâ”€ c.mp4
   â”œâ”€ rf.mp4
   â””â”€ y.mp4

```

